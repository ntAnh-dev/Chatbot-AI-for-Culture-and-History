import time
import logging
from newspaper import Article
from googlesearch import search
from sentence_transformers import SentenceTransformer, util

# Load BGE-M3 model
model = SentenceTransformer("BAAI/bge-m3")

logging.basicConfig(filename="crawl_errors.log", level=logging.WARNING)

def extract_article_safe(url):
    try:
        article = Article(url, language='vi')
        article.download()
        article.parse()
        return article.text
    except Exception as e:
        logging.warning(f"‚ùå Fail at {url}: {e}")
        return None

def chunk_text(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def search_and_get_top_2_chunks(query):
    try:
        urls = list(search(query, num_results=1))
    except Exception as e:
        print(f"‚ùå L·ªói khi t√¨m ki·∫øm: {e}")
        return []

    if not urls:
        print("‚ùó Kh√¥ng t√¨m th·∫•y URL n√†o.")
        return []

    url = urls[0]
    print(f"üìÑ ƒêang l·∫•y n·ªôi dung t·ª´: {url}")
    text = extract_article_safe(url)

    if not text or len(text.strip()) < 300:
        print("‚ùå N·ªôi dung kh√¥ng ƒë·ªß d√†i.")
        return []

    # T√°ch chunk
    chunks = chunk_text(text, chunk_size=500, overlap=50)

    # T√≠nh embedding v√† similarity
    query_emb = model.encode(query, convert_to_tensor=True)
    chunk_embs = model.encode(chunks, convert_to_tensor=True)
    scores = util.cos_sim(query_emb, chunk_embs)[0]

    top_indices = scores.topk(k=2).indices

    results = []
    for idx in top_indices:
        idx = idx.item()
        results.append({
            "content": chunks[idx],
            "similarity": float(scores[idx]),
            "source": url
        })

    return results
